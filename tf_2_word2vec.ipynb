{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# revised version from https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/04_word2vec_visualize.py\n",
    "import codecs\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        # self.global_step is a counter, so it should not be trained (i.e., trainable=False)\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        \"\"\" Step 1: define the placeholders for input and output \"\"\"\n",
    "        with tf.name_scope('input_data'):\n",
    "            self.center_words = tf.placeholder(dtype=tf.int32, shape=[self.batch_size], name='center_words')\n",
    "            self.target_words = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, 1], name='target_words')\n",
    "\n",
    "    def _create_embedding(self):\n",
    "        \"\"\" Step 2: define weights. In word2vec, it's actually the weights that we care about \"\"\"\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.embed_matrix = tf.Variable(initial_value=tf.random_uniform(shape=[self.vocab_size, self.embed_size], minval=-1.0, maxval=1.0), name='embed_matrix')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\" Step 3 + 4: define the model + the loss function \"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            # Step 3: define the inference\n",
    "            # directly get the embedding of 'ids'\n",
    "            # see https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do/41922877\n",
    "            embed = tf.nn.embedding_lookup(params=self.embed_matrix, ids=self.center_words, name='embed')\n",
    "\n",
    "            # Step 4: define loss function\n",
    "            # the results showed that sampled_softmax_loss is litter better than nce_loss\n",
    "            softmax_weights = tf.Variable(initial_value=tf.truncated_normal([self.vocab_size, self.embed_size], stddev=1.0 / self.embed_size ** 0.5), name='sampled_softmax_weight')\n",
    "            softmax_biases = tf.Variable(initial_value=tf.zeros([self.vocab_size]), name='sampled_softmax_bias')\n",
    "            self.loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                                                  labels=self.target_words, num_sampled=NUM_SAMPLED, num_classes=self.vocab_size, name='sampled_softmax_loss'))\n",
    "            '''\n",
    "            # construct variables for NCE loss\n",
    "            nce_weight = tf.Variable(initial_value=tf.truncated_normal(shape=[self.vocab_size, self.embed_size], stddev=1.0 / (self.embed_size ** 0.5)), name='nce_weight')\n",
    "            nce_bias = tf.Variable(initial_value=tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "\n",
    "            # define loss function to be NCE loss function\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                      biases=nce_bias,\n",
    "                                                      labels=self.target_words,\n",
    "                                                      inputs=embed,\n",
    "                                                      num_sampled=self.num_sampled,\n",
    "                                                      num_classes=self.vocab_size), name='nce_loss')\n",
    "            '''\n",
    "\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\" Step 5: define optimizer \"\"\"\n",
    "        # do not forget global_step parameter\n",
    "        self.optimizer = tf.train.AdagradOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "        #self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar(name='loss', tensor=self.loss)\n",
    "            tf.summary.histogram(name='histogram_loss', values=self.loss)\n",
    "            # merge all summaries into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input and generate batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_articles, vocab_size):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    print('Corpus size: {}'.format(len(all_articles)))\n",
    "    words = list(itertools.chain.from_iterable(all_articles)) # flatten all articles into one article\n",
    "    word2id = dict()\n",
    "    word_count = [('UNK', -1)]\n",
    "    word_count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    print('Total number of unique words: {}'.format(len(Counter(words).most_common())))\n",
    "\n",
    "    idx = 0 # the word index of 'UNK' is now 0    \n",
    "    # save most frequent words for TensorBoard visualization\n",
    "    most_freq_words = codecs.open(filename='processed/vocab_' + str(VOCAB_SIZE) + '.tsv', mode='w', encoding='utf-8')\n",
    "    for word, freq in word_count:\n",
    "        word2id[word] = idx\n",
    "        if idx < VOCAB_SIZE:\n",
    "            most_freq_words.write(word + '\\n')\n",
    "        idx += 1\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys())) # convenient convert word2id to id2word\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def convert_words_to_index(all_articles, dictionary):\n",
    "    \"\"\" Replace each word in the corpus with its index in the dictionary \"\"\"\n",
    "    word_index_list = []\n",
    "    for each_article in all_articles:\n",
    "        word_index_list.append([dictionary[word] if word in dictionary else 0 for word in each_article]) # if the word index is larger than VOCAB_SIZE, then replace it with 0\n",
    "    return word_index_list\n",
    "\n",
    "def generate_sample(index_words_list, context_window_size, index_dictionary):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for outer_index, id_words in enumerate(index_words_list): # iterate each article\n",
    "        for inner_index, center in enumerate(id_words): # inside each article\n",
    "            context = random.randint(1, context_window_size)\n",
    "            if outer_index == 0: # print the first article\n",
    "                print('random context:{}'.format(context))\n",
    "            # get a random target before the center word\n",
    "            for target in id_words[max(0, inner_index-context): inner_index]:\n",
    "                if outer_index == 0: # print the first article\n",
    "                    print('before the center word === {} vs {} and {} vs {}'.format(center, target, index_dictionary[center], index_dictionary[target]))\n",
    "                yield center, target\n",
    "            # get a random target after the center word\n",
    "            for target in id_words[(inner_index+1):(inner_index+context+1)]:\n",
    "                if outer_index == 0:\n",
    "                    print('after the center word === {} vs {} and {} vs {}'.format(center, target, index_dictionary[center], index_dictionary[target]))\n",
    "                yield center, target\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "def process_data(vocab_size, batch_size, skip_window_size):\n",
    "    all_articles = read_data(file_path=INPUT_FILE)\n",
    "    word2id, id2word = build_vocab(all_articles, vocab_size)\n",
    "    index_words_list = convert_words_to_index(all_articles, word2id)\n",
    "    del all_articles # to save memory\n",
    "    single_gen = generate_sample(index_words_list, skip_window_size, id2word)\n",
    "    return get_batch(single_gen, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path) # delete an entire directory tree\n",
    "    os.mkdir(path)\n",
    "    \n",
    "def read_data(file_path):\n",
    "    \"\"\" read the input corpus \"\"\"\n",
    "    all_articles = []\n",
    "    for line in codecs.open(filename=file_path, mode='r', encoding='utf-8'):\n",
    "        all_articles.append(line.split())\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_gen, num_train_steps):\n",
    "    \"\"\" start to train the model \"\"\"\n",
    "    saver = tf.train.Saver() # defaults to saving all variables    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        writer = tf.summary.FileWriter(logdir='improved_graph/lr' + str(LEARNING_RATE), graph=sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        for idx in range(initial_step, initial_step+num_train_steps):\n",
    "            centers, targets = next(batch_gen)\n",
    "            #print('Centers shape:{}, targets shape:{}'.format(centers.shape, targets.shape))\n",
    "            feed_dict = {model.center_words: centers, model.target_words: targets}\n",
    "            batch_loss, _, batch_sum = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "            writer.add_summary(summary=batch_sum, global_step=idx)\n",
    "            total_loss += batch_loss\n",
    "            if (idx + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(idx, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "                saver.save(sess=sess, save_path='checkpoints/skip_gram', global_step=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    make_dir('checkpoints')\n",
    "    make_dir('processed')\n",
    "    make_dir('improved_graph')\n",
    "    \n",
    "    sg_model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    sg_model.build_graph()\n",
    "    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW_SIZE)\n",
    "    train_model(sg_model, batch_gen, NUM_TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 108000\n",
      "Total number of unique words: 331437\n",
      "random context:1\n",
      "after the center word === 2 vs 6 and 月 vs 日\n",
      "random context:1\n",
      "before the center word === 6 vs 2 and 日 vs 月\n",
      "after the center word === 6 vs 6450 and 日 vs 那场\n",
      "random context:1\n",
      "before the center word === 6450 vs 6 and 那场 vs 日\n",
      "after the center word === 6450 vs 4551 and 那场 vs 雨\n",
      "random context:1\n",
      "before the center word === 4551 vs 6450 and 雨 vs 那场\n",
      "after the center word === 4551 vs 17840 and 雨 vs 挡住\n",
      "random context:1\n",
      "before the center word === 17840 vs 4551 and 挡住 vs 雨\n",
      "after the center word === 17840 vs 7073 and 挡住 vs 聚会\n",
      "random context:1\n",
      "before the center word === 7073 vs 17840 and 聚会 vs 挡住\n",
      "after the center word === 7073 vs 116007 and 聚会 vs 群主\n",
      "random context:1\n",
      "before the center word === 116007 vs 7073 and 群主 vs 聚会\n",
      "after the center word === 116007 vs 166 and 群主 vs 管理\n",
      "random context:1\n",
      "before the center word === 166 vs 116007 and 管理 vs 群主\n",
      "after the center word === 166 vs 106240 and 管理 vs 海鲜大餐\n",
      "random context:1\n",
      "before the center word === 106240 vs 166 and 海鲜大餐 vs 管理\n",
      "after the center word === 106240 vs 4551 and 海鲜大餐 vs 雨\n",
      "random context:1\n",
      "before the center word === 4551 vs 106240 and 雨 vs 海鲜大餐\n",
      "after the center word === 4551 vs 19617 and 雨 vs 难得一见\n",
      "random context:1\n",
      "before the center word === 19617 vs 4551 and 难得一见 vs 雨\n",
      "after the center word === 19617 vs 9454 and 难得一见 vs 美景\n",
      "random context:1\n",
      "before the center word === 9454 vs 19617 and 美景 vs 难得一见\n",
      "after the center word === 9454 vs 466 and 美景 vs 吃\n",
      "random context:1\n",
      "before the center word === 466 vs 9454 and 吃 vs 美景\n",
      "after the center word === 466 vs 9454 and 吃 vs 美景\n",
      "random context:1\n",
      "before the center word === 9454 vs 466 and 美景 vs 吃\n",
      "after the center word === 9454 vs 24764 and 美景 vs 姐夫\n",
      "random context:1\n",
      "before the center word === 24764 vs 9454 and 姐夫 vs 美景\n",
      "after the center word === 24764 vs 1227 and 姐夫 vs 开车\n",
      "random context:1\n",
      "before the center word === 1227 vs 24764 and 开车 vs 姐夫\n",
      "after the center word === 1227 vs 117843 and 开车 vs 燕子口\n",
      "random context:1\n",
      "before the center word === 117843 vs 1227 and 燕子口 vs 开车\n",
      "after the center word === 117843 vs 246 and 燕子口 vs 这是\n",
      "random context:1\n",
      "before the center word === 246 vs 117843 and 这是 vs 燕子口\n",
      "after the center word === 246 vs 20664 and 这是 vs 店家\n",
      "random context:1\n",
      "before the center word === 20664 vs 246 and 店家 vs 这是\n",
      "after the center word === 20664 vs 3103 and 店家 vs 特意\n",
      "random context:1\n",
      "before the center word === 3103 vs 20664 and 特意 vs 店家\n",
      "after the center word === 3103 vs 63369 and 特意 vs 李子\n",
      "random context:1\n",
      "before the center word === 63369 vs 3103 and 李子 vs 特意\n",
      "after the center word === 63369 vs 13789 and 李子 vs 好吃\n",
      "random context:1\n",
      "before the center word === 13789 vs 63369 and 好吃 vs 李子\n",
      "after the center word === 13789 vs 582 and 好吃 vs 真的\n",
      "random context:1\n",
      "before the center word === 582 vs 13789 and 真的 vs 好吃\n",
      "after the center word === 582 vs 13789 and 真的 vs 好吃\n",
      "random context:1\n",
      "before the center word === 13789 vs 582 and 好吃 vs 真的\n",
      "after the center word === 13789 vs 0 and 好吃 vs UNK\n",
      "random context:1\n",
      "before the center word === 0 vs 13789 and UNK vs 好吃\n",
      "after the center word === 0 vs 12317 and UNK vs 对不起\n",
      "random context:1\n",
      "before the center word === 12317 vs 0 and 对不起 vs UNK\n",
      "after the center word === 12317 vs 612 and 对不起 vs 照片\n",
      "random context:1\n",
      "before the center word === 612 vs 12317 and 照片 vs 对不起\n",
      "after the center word === 612 vs 9287 and 照片 vs 全是\n",
      "random context:1\n",
      "before the center word === 9287 vs 612 and 全是 vs 照片\n",
      "after the center word === 9287 vs 200 and 全是 vs 手机\n",
      "random context:1\n",
      "before the center word === 200 vs 9287 and 手机 vs 全是\n",
      "after the center word === 200 vs 196086 and 手机 vs 照下来\n",
      "random context:1\n",
      "before the center word === 196086 vs 200 and 照下来 vs 手机\n",
      "after the center word === 196086 vs 32508 and 照下来 vs 没带\n",
      "random context:1\n",
      "before the center word === 32508 vs 196086 and 没带 vs 照下来\n",
      "after the center word === 32508 vs 0 and 没带 vs UNK\n",
      "random context:1\n",
      "before the center word === 0 vs 32508 and UNK vs 没带\n",
      "after the center word === 0 vs 4682 and UNK vs 这张\n",
      "random context:1\n",
      "before the center word === 4682 vs 0 and 这张 vs UNK\n",
      "after the center word === 4682 vs 612 and 这张 vs 照片\n",
      "random context:1\n",
      "before the center word === 612 vs 4682 and 照片 vs 这张\n",
      "after the center word === 612 vs 33877 and 照片 vs 南瓜\n",
      "random context:1\n",
      "before the center word === 33877 vs 612 and 南瓜 vs 照片\n",
      "after the center word === 33877 vs 2761 and 南瓜 vs 像是\n",
      "random context:1\n",
      "before the center word === 2761 vs 33877 and 像是 vs 南瓜\n",
      "after the center word === 2761 vs 19291 and 像是 vs 海鲜\n",
      "random context:1\n",
      "before the center word === 19291 vs 2761 and 海鲜 vs 像是\n",
      "after the center word === 19291 vs 2310 and 海鲜 vs 看着\n",
      "random context:1\n",
      "before the center word === 2310 vs 19291 and 看着 vs 海鲜\n",
      "after the center word === 2310 vs 13789 and 看着 vs 好吃\n",
      "random context:1\n",
      "before the center word === 13789 vs 2310 and 好吃 vs 看着\n",
      "after the center word === 13789 vs 1866 and 好吃 vs 谢谢\n",
      "random context:1\n",
      "before the center word === 1866 vs 13789 and 谢谢 vs 好吃\n",
      "after the center word === 1866 vs 2 and 谢谢 vs 月\n",
      "random context:1\n",
      "before the center word === 2 vs 1866 and 月 vs 谢谢\n",
      "after the center word === 2 vs 1250 and 月 vs 听\n",
      "random context:1\n",
      "before the center word === 1250 vs 2 and 听 vs 月\n",
      "after the center word === 1250 vs 3722 and 听 vs 风\n",
      "random context:1\n",
      "before the center word === 3722 vs 1250 and 风 vs 听\n",
      "after the center word === 3722 vs 166 and 风 vs 管理\n",
      "random context:1\n",
      "before the center word === 166 vs 3722 and 管理 vs 风\n",
      "after the center word === 166 vs 23735 and 管理 vs 雾\n",
      "random context:1\n",
      "before the center word === 23735 vs 166 and 雾 vs 管理\n",
      "after the center word === 23735 vs 48453 and 雾 vs 山中\n",
      "random context:1\n",
      "before the center word === 48453 vs 23735 and 山中 vs 雾\n",
      "after the center word === 48453 vs 16693 and 山中 vs 升起\n",
      "random context:1\n",
      "before the center word === 16693 vs 48453 and 升起 vs 山中\n",
      "after the center word === 16693 vs 180278 and 升起 vs 两朵花\n",
      "random context:1\n",
      "before the center word === 180278 vs 16693 and 两朵花 vs 升起\n",
      "after the center word === 180278 vs 511 and 两朵花 vs 经历\n",
      "random context:1\n",
      "before the center word === 511 vs 180278 and 经历 vs 两朵花\n",
      "after the center word === 511 vs 10672 and 经历 vs 风雨\n",
      "random context:1\n",
      "before the center word === 10672 vs 511 and 风雨 vs 经历\n",
      "after the center word === 10672 vs 7716 and 风雨 vs 绽放\n",
      "random context:1\n",
      "before the center word === 7716 vs 10672 and 绽放 vs 风雨\n",
      "after the center word === 7716 vs 1760 and 绽放 vs 平时\n",
      "random context:1\n",
      "before the center word === 1760 vs 7716 and 平时 vs 绽放\n",
      "after the center word === 1760 vs 1311 and 平时 vs 水\n",
      "random context:1\n",
      "before the center word === 1311 vs 1760 and 水 vs 平时\n",
      "after the center word === 1311 vs 39823 and 水 vs 小泉\n",
      "random context:1\n",
      "before the center word === 39823 vs 1311 and 小泉 vs 水\n",
      "after the center word === 39823 vs 4320 and 小泉 vs 大雨\n",
      "random context:1\n",
      "before the center word === 4320 vs 39823 and 大雨 vs 小泉\n",
      "after the center word === 4320 vs 55764 and 大雨 vs 欢唱\n",
      "random context:1\n",
      "before the center word === 55764 vs 4320 and 欢唱 vs 大雨\n",
      "after the center word === 55764 vs 62368 and 欢唱 vs 天边\n",
      "random context:1\n",
      "before the center word === 62368 vs 55764 and 天边 vs 欢唱\n",
      "after the center word === 62368 vs 5554 and 天边 vs 云\n",
      "random context:1\n",
      "before the center word === 5554 vs 62368 and 云 vs 天边\n",
      "Average loss at step 1999:   5.2\n",
      "Average loss at step 3999:   4.2\n",
      "Average loss at step 5999:   3.8\n",
      "Average loss at step 7999:   3.5\n",
      "Average loss at step 9999:   3.4\n",
      "Average loss at step 11999:   3.4\n",
      "Average loss at step 13999:   3.2\n",
      "Average loss at step 15999:   3.2\n",
      "Average loss at step 17999:   3.1\n",
      "Average loss at step 19999:   3.0\n",
      "Average loss at step 21999:   3.0\n",
      "Average loss at step 23999:   2.9\n",
      "Average loss at step 25999:   2.9\n",
      "Average loss at step 27999:   2.9\n",
      "Average loss at step 29999:   2.8\n",
      "Average loss at step 31999:   2.8\n",
      "Average loss at step 33999:   2.8\n",
      "Average loss at step 35999:   2.7\n",
      "Average loss at step 37999:   2.7\n",
      "Average loss at step 39999:   2.8\n",
      "Average loss at step 41999:   2.6\n",
      "Average loss at step 43999:   4.2\n",
      "Average loss at step 45999:   3.9\n",
      "Average loss at step 47999:   3.7\n",
      "Average loss at step 49999:   3.5\n",
      "Average loss at step 51999:   3.4\n",
      "Average loss at step 53999:   3.4\n",
      "Average loss at step 55999:   3.3\n",
      "Average loss at step 57999:   3.3\n",
      "Average loss at step 59999:   3.3\n",
      "Average loss at step 61999:   3.2\n",
      "Average loss at step 63999:   3.2\n",
      "Average loss at step 65999:   3.1\n",
      "Average loss at step 67999:   3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 69999:   3.1\n",
      "Average loss at step 71999:   3.1\n",
      "Average loss at step 73999:   3.0\n",
      "Average loss at step 75999:   3.0\n",
      "Average loss at step 77999:   3.0\n",
      "Average loss at step 79999:   4.4\n",
      "Average loss at step 81999:   4.0\n",
      "Average loss at step 83999:   3.8\n",
      "Average loss at step 85999:   3.7\n",
      "Average loss at step 87999:   3.6\n",
      "Average loss at step 89999:   3.5\n",
      "Average loss at step 91999:   3.5\n",
      "Average loss at step 93999:   3.4\n",
      "Average loss at step 95999:   3.4\n",
      "Average loss at step 97999:   3.3\n",
      "Average loss at step 99999:   3.3\n",
      "Average loss at step 101999:   3.3\n",
      "Average loss at step 103999:   3.3\n",
      "Average loss at step 105999:   3.2\n",
      "Average loss at step 107999:   3.2\n",
      "Average loss at step 109999:   3.2\n",
      "Average loss at step 111999:   3.2\n",
      "Average loss at step 113999:   3.1\n",
      "Average loss at step 115999:   3.1\n",
      "Average loss at step 117999:   3.1\n",
      "Average loss at step 119999:   3.1\n",
      "Average loss at step 121999:   3.1\n",
      "Average loss at step 123999:   3.1\n",
      "Average loss at step 125999:   3.0\n",
      "Average loss at step 127999:   3.1\n",
      "Average loss at step 129999:   3.0\n",
      "Average loss at step 131999:   3.0\n",
      "Average loss at step 133999:   3.0\n",
      "Average loss at step 135999:   3.0\n",
      "Average loss at step 137999:   3.0\n",
      "Average loss at step 139999:   3.0\n",
      "Average loss at step 141999:   2.8\n",
      "Average loss at step 143999:   2.1\n",
      "Average loss at step 145999:   1.9\n",
      "Average loss at step 147999:   1.9\n",
      "Average loss at step 149999:   1.8\n",
      "Average loss at step 151999:   1.8\n",
      "Average loss at step 153999:   1.7\n",
      "Average loss at step 155999:   1.7\n",
      "Average loss at step 157999:   1.6\n",
      "Average loss at step 159999:   1.8\n",
      "Average loss at step 161999:   1.7\n",
      "Average loss at step 163999:   1.7\n",
      "Average loss at step 165999:   1.6\n",
      "Average loss at step 167999:   1.7\n",
      "Average loss at step 169999:   1.7\n",
      "Average loss at step 171999:   1.7\n",
      "Average loss at step 173999:   1.6\n",
      "Average loss at step 175999:   1.7\n",
      "Average loss at step 177999:   3.2\n",
      "Average loss at step 179999:   3.0\n",
      "Average loss at step 181999:   2.7\n",
      "Average loss at step 183999:   2.4\n",
      "Average loss at step 185999:   2.2\n",
      "Average loss at step 187999:   2.1\n",
      "Average loss at step 189999:   2.0\n",
      "Average loss at step 191999:   1.9\n",
      "Average loss at step 193999:   1.9\n",
      "Average loss at step 195999:   1.8\n",
      "Average loss at step 197999:   1.8\n",
      "Average loss at step 199999:   4.4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # hyper-parameters\n",
    "    VOCAB_SIZE = 200000 # the considered size of words\n",
    "    BATCH_SIZE = 128\n",
    "    EMBED_SIZE = 128  # dimension of the word embedding vectors\n",
    "    SKIP_WINDOW_SIZE = 1  # the context window\n",
    "    NUM_SAMPLED = 64  # Number of negative examples to sample.\n",
    "    LEARNING_RATE = 0.5\n",
    "    NUM_TRAIN_STEPS = 200000\n",
    "    SKIP_STEP = 2000\n",
    "\n",
    "    INPUT_FILE = 'E:/2017_Deep_learning/word2vec/word_vector_108000.cs'\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
